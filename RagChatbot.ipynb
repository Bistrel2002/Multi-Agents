{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad9be093",
   "metadata": {},
   "source": [
    "# Data Connector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670eb688",
   "metadata": {},
   "source": [
    "## Phase 1: The Ingestion Loop (Data Preparation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac564e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Langchain imports\n",
    "from  langchain_community.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_pinecone import PineconeSparseVectorStore\n",
    "from pinecone import Pinecone, ServerlessSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97a39dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# 1. CONFIGURATION\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "load_dotenv()  #load from .env file\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "#Ingestion settings\n",
    "\n",
    "DATA_DIR = \"./data\"\n",
    "PINECONE_INDEX_NAME = \"multi-agents\"\n",
    "EMBEDDING_MODEL = \"text-embedding-3-large\"\n",
    "EMBEDDING_DIMENSIONS = 3072\n",
    "\n",
    "CHUNK_SIZE = 700\n",
    "\n",
    "CHUNK_OVERLAP = 70\n",
    "\n",
    "\n",
    "# ─────────────────────────────────────────────\n",
    "# 2. DATA LOADING (PDF Connector)\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "def load_pdfs(data_dir: str) -> list:\n",
    "    \"\"\"\n",
    "    Load all PDF files from the specified directory.\n",
    "    Uses PyPDFLoader to extract text from each page.\n",
    "    \"\"\"\n",
    "    print(\"Step 1: Loading PDF document...\")\n",
    "\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "        print(f\"  Created empty '{data_dir}' folder. Add your PDFs there and re-run\")\n",
    "        return []\n",
    "    \n",
    "    loader = DirectoryLoader(\n",
    "        data_dir,\n",
    "        glob = \"**/*.pdf\",\n",
    "        loader_cls = PyPDFLoader,\n",
    "        show_progress = True\n",
    "    )\n",
    "\n",
    "    documents = loader.load()\n",
    "    print(f' Loaded{len(documents)} pages from PDFs')\n",
    "    return documents\n",
    "\n",
    "\n",
    "# ─────────────────────────────────────────────\n",
    "# 3. DOCUMENT CLEANING\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "def clean_documents(documents: list) -> list:\n",
    "    \"\"\"\n",
    "    Clean loaded documents by removing noise:\n",
    "    - Excessive whitespace\n",
    "    - Page numbers\n",
    "    - Common headers/footers\n",
    "    - Empty pages\n",
    "    \"\"\"\n",
    "\n",
    "    print(\" Step2: Cleaning documents...\")\n",
    "    cleaned = []\n",
    "    removed_count = 0\n",
    "\n",
    "    for doc in documents:\n",
    "        text = doc.page_content\n",
    "\n",
    "        # Remove page numbers\n",
    "        text = re.sub(r'\\n\\s*Page\\s*\\d+\\s*(of\\s*\\d+)?\\s*\\n', '\\n', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'\\n\\s*-\\s*\\d+\\s*-\\s*\\n', '\\n', text)\n",
    "\n",
    "        # Remove excessive newlines (3+ → 2) \n",
    "        text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "\n",
    "        # Remove excessive spaces\n",
    "        text = re.sub(r' {3,}', ' ', text)\n",
    "\n",
    "        # Strip leading/trailing whitespace\n",
    "        text = text.strip()\n",
    "\n",
    "        # Skip nearly empty pages (less than 50 chars of actual content)\n",
    "        if len(text) < 50:\n",
    "            removed_count += 1\n",
    "            continue\n",
    "\n",
    "        doc.page_content = text\n",
    "        cleaned.append(doc)\n",
    "\n",
    "    print(f\" ✅Cleaned {len(cleaned)} pages ({removed_count}) empty pages removed\")\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a680576b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone.db_control.models.index_description import ServerlessSpec\n",
    "# ─────────────────────────────────────────────\n",
    "# 4. CHUNKING (Recursive Character Splitting)\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "def chunk_documents(documents: list) -> list:\n",
    "    \"\"\"\n",
    "    Split documents into smaller chunks using Recursive Character Splitting.\n",
    "    \n",
    "    Strategy:\n",
    "    - Chunk size: 700 tokens \n",
    "    - Overlap: 70 tokens (10%) so context isn't lost at edges\n",
    "    - Splits on paragraphs first, then sentences, then words\n",
    "    \"\"\"\n",
    "\n",
    "    print(\" Step 3: Chunking documents...\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = CHUNK_SIZE,\n",
    "        chunk_overlap = CHUNK_OVERLAP,\n",
    "        length_function = len,\n",
    "        separators = [\n",
    "            \"\\n\\n\",   # Double newline (paragraphs) — preferred split\n",
    "            \"\\n\",     # Single newline\n",
    "            \". \",     # Sentences\n",
    "            \", \",     # Clauses\n",
    "            \" \",      # Words\n",
    "            \"\"        # character (last resort) \n",
    "        ],\n",
    "        is_separator_regex = False,\n",
    "    )\n",
    "\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "  \n",
    "    # Add metadata to each chunk for traceability\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk.metadata[\"chunk_id\"] = i\n",
    "        chunk.metadata[\"chunk_total\"] = len(chunks)\n",
    "\n",
    "        #keep source files name clean\n",
    "        if \"source\" in chunk.metadata:\n",
    "            chunk.metadata[\"source\"] = Path(chunk.metadata[\"source\"]).name\n",
    "\n",
    "    print(f\" ✅Created {len(chunks)} chunks from {len(documents)} pages\")\n",
    "\n",
    "    # Show chunk size statistics\n",
    "    sizes = [len(c.page_content) for c in chunks]\n",
    "    print(f\"Chunk sizes — Min: {min(sizes)}, Max: {max(sizes)}, Avg: {sum(sizes)//len(sizes)}\")\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# ─────────────────────────────────────────────\n",
    "# 5. EMBEDDING & VECTOR STORAGE (Pinecone)\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "\n",
    "def create_pinecone_index(pc: Pinecone) -> None:\n",
    "\n",
    "    \"\"\"\n",
    "    Create the Pinecone index if it doesn't already exist.\n",
    "    \"\"\"\n",
    "    existing_indexes = [idx.name for idx in pc.list_indexes()]\n",
    "    \n",
    "    if PINECONE_INDEX_NAME not in existing_indexes:\n",
    "        print(f\" Creating Pinecone index '{PINECONE_INDEX_NAME}'...\")\n",
    "        pc.create_index(\n",
    "            name=PINECONE_INDEX_NAME,\n",
    "            dimension=EMBEDDING_DIMENSIONS,\n",
    "            metric=\"cosine\",\n",
    "            spec={\n",
    "                \"serverless\": {\n",
    "                    \"cloud\": \"aws\",\n",
    "                    \"region\": \"us-east-1\"\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        print(f\" ✅ Pinecone index '{PINECONE_INDEX_NAME}' created!\")\n",
    "    else:\n",
    "        print(f\" ✅ Pinecone index '{PINECONE_INDEX_NAME}' already exists.\")\n",
    "\n",
    "\n",
    "def embed_and_store(chunks: list) -> PineconeVectorStore:\n",
    "    \"\"\"\n",
    "    Embed all chunks using OpenAI and store them in Pinecone.\n",
    "    \n",
    "    This is the most expensive step (API calls for each chunk).\n",
    "    \"\"\"\n",
    "    print(\"Step 4: Embedding chunks and storing in Pinecone...\")\n",
    "\n",
    "    #Initialize the embedding model\n",
    "    embeddings = OpenAIEmbeddings(\n",
    "        model=EMBEDDING_MODEL,\n",
    "        openai_api_key=OPENAI_API_KEY,\n",
    "    )\n",
    "\n",
    "    #Initialize the Pinecone client\n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "    #Create the index if it doesn't exist\n",
    "    create_pinecone_index(pc)\n",
    "\n",
    "    print(f\" Embedding {len(chunks)} chunks...\")\n",
    "    \n",
    "    # Connect to the vector store\n",
    "    vectorstore = PineconeVectorStore(\n",
    "        index_name=PINECONE_INDEX_NAME,\n",
    "        embedding=embeddings,\n",
    "        pinecone_api_key=PINECONE_API_KEY\n",
    "    )\n",
    "    \n",
    "    # Generate a unique ID for each chunk based on its text\n",
    "    ids = [hashlib.md5(c.page_content.encode('utf-8')).hexdigest() for c in chunks]\n",
    "    \n",
    "    # Add documents using their unique IDs to prevent duplicates\n",
    "    vectorstore.add_documents(documents=chunks, ids=ids)\n",
    "\n",
    "    print(f\" All {len(chunks)} chunks embedded and stored in Pinecone.\")\n",
    "    return vectorstore\n",
    "\n",
    "\n",
    "# ─────────────────────────────────────────────\n",
    "# 6. VERIFICATION (Test Query)\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "def verify_ingestion(vectorstore: PineconeVectorStore) -> None:\n",
    "    \"\"\"\n",
    "    Run a test similarity search to confirm everything works.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n Step 5: Verification — Running test query...\")\n",
    "    \n",
    "    test_query = \"What is the main topic of these documents?\"\n",
    "    results = vectorstore.similarity_search(test_query, k=3)\n",
    "\n",
    "    print(f\"\\n✅Test query returned {len(results)} results!\\n\")\n",
    "\n",
    "    for i, result in enumerate(results, 1):\n",
    "        source = result.metadata.get(\"source_file\", \"Unknown\")\n",
    "        preview = result.page_content[:150].replace(\"\\n\", \" \")\n",
    "        print(f\" Result{i}. from{source}:\")\n",
    "        print(f\".  \\\"{preview}...\\\"\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "877fc78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MULTI-AGENTS— INGESTION PIPELINE\n",
      "================================================================================\n",
      "\n",
      "Config:\n",
      "   Data directory:  ./data\n",
      "   Embedding model: text-embedding-3-large\n",
      "   Chunk size:      700 chars, 70 overlap\n",
      "   Pinecone index:  multi-agents\n",
      "\n",
      "Step 1: Loading PDF document...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:04<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded103 pages from PDFs\n",
      " Step2: Cleaning documents...\n",
      " ✅Cleaned 101 pages (2) empty pages removed\n",
      " Step 3: Chunking documents...\n",
      " ✅Created 300 chunks from 101 pages\n",
      "Chunk sizes — Min: 27, Max: 700, Avg: 567\n",
      "Step 4: Embedding chunks and storing in Pinecone...\n",
      " Creating Pinecone index 'multi-agents'...\n",
      " ✅ Pinecone index 'multi-agents' created!\n",
      " Embedding 300 chunks...\n",
      " All 300 chunks embedded and stored in Pinecone.\n",
      "\n",
      " Step 5: Verification — Running test query...\n",
      "\n",
      "✅Test query returned 3 results!\n",
      "\n",
      " Result1. fromUnknown:\n",
      ".  \"met. Technical writer Jason GrossoDocuments features, creates user guides, and maintains project documentation. I. Introduction Current situation: \"Gr...\"\n",
      "\n",
      " Result2. fromUnknown:\n",
      ".  \"8GDPR (General Data Protection Regulation): This is a regulation in EU law that protects the privacy and personal data of EU citizens. It gives indivi...\"\n",
      "\n",
      " Result3. fromUnknown:\n",
      ".  \"Functional Specifiactions | Team 6 Table of Contents 1. Document handling 1.1 Document information Document Type SpecificationDocument OwnerRaphaël De...\"\n",
      "\n",
      "================================================================================\n",
      " ✅ INGESTION COMPLETE! Your data is ready for Phase 2.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ─────────────────────────────────────────────\n",
    "# 7. MAIN PIPELINE\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "def run_ingestion_pipeline():\n",
    "    \"\"\"\n",
    "    Execute the full ingestion pipeline:\n",
    "    Load → Clean → Chunk → Embed → Store → Verify\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"MULTI-AGENTS— INGESTION PIPELINE\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "\n",
    "    # VALIDATE API KEYS\n",
    "    if not OPENAI_API_KEY:\n",
    "        raise ValueError(\"❌OPENAI_API_KEY is not set. Add it to your .env file.\")\n",
    "    if not PINECONE_API_KEY:\n",
    "        raise ValueError(\"❌PINECONE_API_KEY is not set. Add it to your .env file.\")\n",
    "    \n",
    "    print(f\"Config:\")\n",
    "    print(f\"   Data directory:  {DATA_DIR}\")\n",
    "    print(f\"   Embedding model: {EMBEDDING_MODEL}\")\n",
    "    print(f\"   Chunk size:      {CHUNK_SIZE} chars, {CHUNK_OVERLAP} overlap\")\n",
    "    print(f\"   Pinecone index:  {PINECONE_INDEX_NAME}\")\n",
    "    print()\n",
    "\n",
    "    #Step 1: Load PDFs\n",
    "    documents = load_pdfs(DATA_DIR)\n",
    "    if not documents:\n",
    "        print(\"❌No documents found. Add PDFs to the 'data/' folder.\")\n",
    "        return\n",
    "    \n",
    "    #Step 2: Clean\n",
    "    documents =  clean_documents(documents)\n",
    "\n",
    "    #Step 3: Chunk\n",
    "    chunks = chunk_documents(documents)\n",
    "\n",
    "    #step 4 & 5: Embed and store\n",
    "    vectorstore = embed_and_store(chunks)\n",
    "\n",
    "    #step 6: Verify\n",
    "    verify_ingestion(vectorstore)\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\" ✅ INGESTION COMPLETE! Your data is ready for Phase 2.\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_ingestion_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cd8670",
   "metadata": {},
   "source": [
    "## Phase 2: The Inference Loop (The Chatbot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f57c875",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "# ─────────────────────────────────────────────\n",
    "# 1. QUERY TRANSFORMATION\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "# Initialize the LLM for query transformation\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-4o\", \n",
    "    temperature=0. # Deterministic output for consistent rewrites\n",
    ")\n",
    "\n",
    "# Query transform prompt\n",
    "query_transform_prompt =  ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a query transformation assistant. Your job is to rewrite \n",
    "the user's question to be more specific and search-friendly for a vector database search.\n",
    "\n",
    "Rules: \n",
    "- Expand abbreviations and acronyms if possible\n",
    "- Add relevant context words\n",
    "- Make the query self-contained (don't assume context)\n",
    "- Keep it concise but specific\n",
    "- Return ONLY the rewritten query, nothing else\"\"\"),\n",
    "    (\"human\", \"Output query: {query}\\n\\nRewritten query: \")\n",
    "])\n",
    "    \n",
    "query_transform_chain = query_transform_prompt | llm\n",
    "\n",
    "def transform_query(user_query: str) -> str:\n",
    "    \"\"\"\n",
    "    Rewrite the user's query to be more search-friendly.\n",
    "    \"\"\"\n",
    "    result = query_transform_chain.invoke({\"query\": user_query})\n",
    "    transformed = result.content.strip()\n",
    "    return transformed\n",
    "\n",
    "\n",
    "# ─────────────────────────────────────────────\n",
    "# 2. RETRIEVAL(HYBRID SEARCH)\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "\n",
    "# ── Connect to your existing Pinecone index ──\n",
    "def connect_to_vectorstore() -> PineconeVectorStore:\n",
    "    \"\"\"\n",
    "    Connect to the existing Pinecone index created in Phase 1.\n",
    "    No re-embedding needed — we're just connecting.\n",
    "    \"\"\"\n",
    "\n",
    "    embeddings = OpenAIEmbeddings(\n",
    "        model=EMBEDDING_MODEL,\n",
    "        openai_api_key=OPENAI_API_KEY,\n",
    "    )\n",
    "    vectorstore = PineconeVectorStore(\n",
    "        index_name=PINECONE_INDEX_NAME,\n",
    "        embedding=embeddings,\n",
    "        pinecone_api_key=PINECONE_API_KEY,\n",
    "    )\n",
    "\n",
    "    print(f\" ✅ Connected to Pinecone index '{PINECONE_INDEX_NAME}'\")\n",
    "    return vectorstore\n",
    "\n",
    "# ── Retrieval function ──\n",
    "# def retrieve_documents(vectorstore: PineconeVectorStore, query: str, k: int = 20): \n",
    "#     \"\"\"\n",
    "#     Retrieve the top-k most relevant chunks using similarity search.\n",
    "#     We fetch 20 results here (not 5) because the Reranker in Step 3 \n",
    "#     will narrow it down to the top 5.\n",
    "#     \"\"\"\n",
    "\n",
    "#     print(f\"\\n Step 2: Retrieving top {k} relevant documents\")\n",
    "#     retrieved_docs = vectorstore.similarity_search(query, k=k)\n",
    "\n",
    "#     print(f\" ✅ Retrieved {len(retrieved_docs)} candidate chunks\")\n",
    "#     for i, doc in enumerate(retrieved_docs[:5], 1): #Preview top 5\n",
    "#         source = doc.metadata.get(\"Source\", \"Unknown\")\n",
    "#         preview = doc.page_content[:80].replace(\"\\n\", \" \")\n",
    "#         print(f\" [{i}] (score: {source}: \\\"{preview}...\\\"\")\n",
    "    \n",
    "#     return retrieved_docs\n",
    "\n",
    "\n",
    "# ─────────────────────────────────────────────\n",
    "# 3. RERANKING (THE MAIN SOURCE)\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "from langchain_cohere import CohereRerank\n",
    "from langchain_classic.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "\n",
    "COHERE_API_KEY = os.getenv(\"COHERE_API_KEY\")\n",
    "\n",
    "def rerank_documents(vectorstore, query: str, top_n: int = 5):\n",
    "    \"\"\"\n",
    "    Use Cohere Reranker to re-score and select the top-n most relevant chunks.  \n",
    "    Flow: 20 candidates → Cross-Encoder scoring → Top 5 winners\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"\\n Step 3: Reranking to find top {top_n} results...\")\n",
    "\n",
    "    # Create base retriever (fetches 20 candidates)\n",
    "    base_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 20})\n",
    "\n",
    "    # Create the ranker\n",
    "    reranker = CohereRerank(\n",
    "        cohere_api_key = COHERE_API_KEY,\n",
    "        model = \"rerank-v3.5\",\n",
    "        top_n = top_n\n",
    "    )\n",
    "\n",
    "    # Wrap retriever with reranker\n",
    "    compression_retriever = ContextualCompressionRetriever(\n",
    "        base_compressor = reranker,\n",
    "        base_retriever = base_retriever\n",
    "    )\n",
    "\n",
    "    # Get reranked results\n",
    "    reranked_docs = compression_retriever.invoke(query)\n",
    "\n",
    "    print(f\"  ✅ Reranked to top {len(reranked_docs)} chunks\")\n",
    "    for i, doc in enumerate(reranked_docs, 1):\n",
    "        source = doc.metadata.get(\"source\", \"Unknown\")\n",
    "        score = doc.metadata.get(\"relevance_score\", \"N/A\")\n",
    "        preview = doc.page_content[:80].replace(\"\\n\", \" \")\n",
    "        print(f\"  [{i}] (relevance: {score}) {source}: \\\"{preview}...\\\"\")\n",
    "    \n",
    "    return reranked_docs\n",
    "\n",
    "\n",
    "# ─────────────────────────────────────────────\n",
    "# 4. CONTEXT ASSEMBLY & GENERATION\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a corporate assistant for the Multi-Agents project. \n",
    "    Your job is to answer questions ONLY using the provided context documents.\n",
    "\n",
    "    STRICT RULES:\n",
    "    1. Answer ONLY using the provided context. Do NOT use prior knowledge.\n",
    "    2. If the answer is not in the context, say: \"I do not have enough information to answer this based on the available documents.\"\n",
    "    3. Always cite which document your answer comes from (e.g., \"Based on [document name]...\").\n",
    "    4. Be concise, professional, and precise.\n",
    "    5. If the context contains conflicting information, mention both sources and the discrepancy.\n",
    "    \n",
    "    CONTEXT DOCUMENTS:\n",
    "    {context}\n",
    "    \"\"\"),\n",
    "        (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "# The generation LLM\n",
    "generation_llm = ChatOpenAI(\n",
    "    model = \"gpt-4o\",\n",
    "    openai_api_key = OPENAI_API_KEY,\n",
    "    temperature = 0.1 # low temperature for factual accuracy\n",
    ")\n",
    "\n",
    "rag_chain = rag_prompt | generation_llm | StrOutputParser()\n",
    "\n",
    "\n",
    "def format_context(docs) -> str:\n",
    "    \"\"\"\n",
    "    Format the retrieved documents into a clean context string\n",
    "    with source attribution.\n",
    "    \"\"\"\n",
    "\n",
    "    context_parts = []\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        source = doc.metadata.get(\"source\", \"Unknown\")\n",
    "        page = doc.metadata.get(\"page\", \"N/A\")\n",
    "        context_parts.append(\n",
    "            f\"[Document {i} | Source: {source} | Page: {page}]\\n{doc.page_content}\"\n",
    "        )\n",
    "    return \"\\n\\n---\\n\\n\".join(context_parts)\n",
    "\n",
    "def generate_answer(docs, question: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate the final answer using the reranked documents and the user's question.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"\\n Step 4: Generating answer...\")\n",
    "\n",
    "    context = format_context(docs)\n",
    "    \n",
    "    answer = rag_chain.invoke({\n",
    "        \"context\": context,\n",
    "        \"question\": question\n",
    "    })\n",
    "    \n",
    "    print(f\"  ✅ Answer generated!\\n\")\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2a7e7eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "❓ USER QUESTION: According to the document what is the current version and who is the author of locindoor\n",
      "================================================================================\n",
      " ✅ Connected to Pinecone index 'multi-agents'\n",
      "\n",
      " Step 3: Reranking to find top 5 results...\n",
      "  ✅ Reranked to top 5 chunks\n",
      "  [1] (relevance: 0.17046688) locindoor.pdf: \"Functional Specification of LOC-INDOOR (Indoor Localisation) Contents \u0000 Line-Bas...\"\n",
      "  [2] (relevance: 0.023529684) locindoor.pdf: \"Postconditions: User completes all planned destinations System logs complete jou...\"\n",
      "  [3] (relevance: 0.022322435) locindoor.pdf: \"iBeacon: Apple's protocol for Bluetooth Low Energy (BLE) proximity sensing that ...\"\n",
      "  [4] (relevance: 0.02211875) locindoor.pdf: \"Navigation 10.2 UI Components 10.2.1 Information Displays Distance remaining Est...\"\n",
      "  [5] (relevance: 0.02064795) locindoor.pdf: \"Floor detection 9.1.5 Map Rendering 3D Map Engine Purpose: Displays venue maps a...\"\n",
      "\n",
      " Step 4: Generating answer...\n",
      "  ✅ Answer generated!\n",
      "\n",
      "================================================================================\n",
      "ANSWER:\n",
      "================================================================================\n",
      "Based on [Document 1 | Source: locindoor.pdf | Page: 0.0], the current version of the document is 1.1, and the author is TSANGUE VIVIEN BISTREL.\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Based on [Document 1 | Source: locindoor.pdf | Page: 0.0], the current version of the document is 1.1, and the author is TSANGUE VIVIEN BISTREL.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ask(question: str) -> str:\n",
    "    \"\"\"\n",
    "    The full RAG inference pipeline:\n",
    "    Query Transform → Retrieve → Rerank → Generate\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"❓ USER QUESTION: {question}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Step 0: Connect to vector store\n",
    "    vectorstore = connect_to_vectorstore()\n",
    "\n",
    "    # Step 1: Transform the query\n",
    "    transformed_query = transform_query(question)\n",
    "\n",
    "    # Step 2: Retrieve candidates (top 20)\n",
    "    # results = retrieve_documents(vectorstore, transformed_query, k=20)\n",
    "\n",
    "    #  Step 2 & 3:Retrieve & Rerank to top 5\n",
    "    top_docs = rerank_documents(vectorstore, transformed_query, top_n=5)\n",
    "\n",
    "    # Step 4: Generate answer\n",
    "    answer = generate_answer(top_docs, question)\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"ANSWER:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(answer)\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# ── Test ──\n",
    "ask(\"According to the document what is the current version and who is the author of locindoor\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3179bcce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
